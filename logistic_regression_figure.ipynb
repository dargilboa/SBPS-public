{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression - comparing samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from samplers import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#np.random.seed(0)\n",
    "#tf.set_random_seed(0)\n",
    "from sklearn import linear_model\n",
    "import copy\n",
    "import scipy.stats\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dar data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate logistic regression data\n",
    "D =   20\n",
    "N = 1000\n",
    "target_w = 10*(np.random.rand(D) - .5)\n",
    "train_images = np.random.rand(N,D) - .5\n",
    "cov = np.diag(np.ones(D))\n",
    "cov[0,0] = 6\n",
    "for i in range(N):\n",
    "    train_images[i,:] = np.random.multivariate_normal(np.zeros(D),cov)\n",
    "base=np.float32\n",
    "tfbase=tf.float32\n",
    "train_probs = logit(np.dot(train_images,target_w))\n",
    "train_labels = np.expand_dims(train_probs<np.random.rand(N),1).astype(base)\n",
    "xData = np.float32( train_images.reshape( train_labels.shape[0], train_images[0].size ) )\n",
    "yData = train_labels\n",
    "probs=logit(np.matmul(train_images, target_w))\n",
    "print((NLLexplicit(probs,train_probs)))\n",
    "\n",
    "NLLexplicit(probs,train_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf]\n",
      "[[ 0.32002982  1.39770253 -0.97311093  1.39519925 -0.21466235 -0.99232534\n",
      "   1.42017851  0.9662785   0.84334037  1.00422016  0.90446579  0.41954094\n",
      "   0.89402187  0.40207118 -0.40824313 -0.51620558 -0.54209306  0.67453355\n",
      "  -0.00803189  0.34644805]]\n"
     ]
    }
   ],
   "source": [
    "# Use SKLearn to run SAG\n",
    "import sklearn.linear_model as lin_mod\n",
    "maxNumEpochs=5000;\n",
    "skLogistic=lin_mod.LogisticRegression(fit_intercept=False,solver='sag',C=1e10,max_iter=N*10,tol=1e-6)\n",
    "skLogistic.fit(xData,np.reshape(yData,[N,]))\n",
    "W_map=skLogistic.coef_.copy()\n",
    "probs_sk=logit(np.matmul(train_images, np.reshape(W_map,[D,1])))\n",
    "print((NLLexplicit(probs_sk,np.reshape(yData,[N]))))\n",
    "print(W_map-target_w) # show differences in W\n",
    "W_map = np.transpose(W_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "xDataPH = tf.placeholder(tfbase, [None,D])\n",
    "yDataPH = tf.placeholder(tfbase, [None,1],)\n",
    "#yClass = tf.placeholder(tf.float32, [None, 10])\n",
    "W = tf.Variable(tf.zeros([D,1],tfbase),tfbase)\n",
    "linProd= tf.matmul(xDataPH,W)\n",
    "objective=tf.nn.sigmoid_cross_entropy_with_logits(linProd,yDataPH)\n",
    "NLL=tf.reduce_mean(objective)\n",
    "# Create an ExponentialMovingAverage object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define operations for custom learning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# operation to calculate gradient and return gradient\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=1)\n",
    "grads_and_vars = opt.compute_gradients(NLL)\n",
    "gradients, variables = zip(*grads_and_vars)\n",
    "g=tf.reshape(gradients[0],[D])\n",
    "weights = tf.reshape(variables[0],[D])\n",
    "ghostGradient = tf.placeholder(tfbase,[D])\n",
    "grads_to_apply = []\n",
    "grads_to_apply.append(tf.reshape(ghostGradient,[D,1]))\n",
    "grad_and_vars_to_apply=list(zip(grads_to_apply, variables))\n",
    "train_step = opt.apply_gradients(grad_and_vars_to_apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit gradients so that variance can be calculate for the GP\n",
    "#### Note this is *not* general purpose and will not work if the model statement is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "velocity_dummy=tf.placeholder(tfbase,[D])\n",
    "yPred=tf.sigmoid(linProd)\n",
    "xPHR=tf.reshape(xDataPH,[-1,D])\n",
    "gradient_explicit=-(yDataPH-yPred)*xPHR\n",
    "gradient_calculation=tf.reduce_mean(gradient_explicit,0)\n",
    "gradient_variance_calculation=tf.reduce_mean(gradient_explicit*gradient_explicit,0) \\\n",
    "    -gradient_calculation*gradient_calculation\n",
    "better_gradient_variance_intermediate=tf.matmul(gradient_explicit,tf.reshape(velocity_dummy,[D,1]))\n",
    "better_gradient_variance_calculation=tf.nn.moments(better_gradient_variance_intermediate,0)[1]\n",
    "hessian_explicit= tf.reshape(yPred*(1-yPred),[-1,1,1])*(tf.reshape(xPHR,[-1,D,1])*tf.reshape(xPHR,[-1,1,D]))\n",
    "hessian_calculation=tf.reduce_mean(hessian_explicit,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method to calculate accuracy from a minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(yDataPH, tf.cast(.5+.5*tf.sign(tf.matmul(xDataPH, W)),tfbase))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tfbase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hess = np.zeros((D,D))\n",
    "for i in range(N):\n",
    "    ex = np.exp(np.dot(np.squeeze(W_map),train_images[i,:]))\n",
    "    hess += (ex/float((1+ex)**2))*np.outer(train_images[i,:],train_images[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to initialize session and run optimization/sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runOptimizationExplicit(my_updater,use_preconditioner,W_map=0):\n",
    "    start = time.time()\n",
    "    # initialize tf for optimization\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    lam=1e-4\n",
    "    beta1=.99\n",
    "    grad2=np.zeros(D)\n",
    "    preconditioner = 0\n",
    "    perrs = []\n",
    "    # how long to run optimization\n",
    "    N=len(xData)\n",
    "    # iteration counter\n",
    "    iter=0\n",
    "    # initialize variables to store history\n",
    "    NLL_factor = 5\n",
    "    n_i = 0\n",
    "    NLL_store = np.zeros(total_iter/NLL_factor)\n",
    "    acc_store=np.zeros(total_iter)\n",
    "    samples = np.zeros((total_iter,D))\n",
    "    if not type(W_map) == int:\n",
    "        wset=W.assign(W_map)\n",
    "        with sess.as_default():\n",
    "            sess.run(wset)\n",
    "        #print 'Initial position assigned, 0 coordinate is  - ', W_map[0]\n",
    "        \n",
    "    if not use_preconditioner:\n",
    "        preconditioner = 1\n",
    "    # main optimization loop\n",
    "    for n in xrange(n_epochs):\n",
    "        for minibatch in iterate_minibatches(xData, yData, batch_size, shuffle=True):\n",
    "            \n",
    "            # grab new minibatch\n",
    "            x_batch, y_batch = minibatch\n",
    "            \n",
    "            # calculate gradient\n",
    "            if my_updater.__class__ == lSBPS:\n",
    "                velocity_input=my_updater.v\n",
    "                with sess.as_default():\n",
    "                    gradient=gradient_calculation.eval(feed_dict={xDataPH: x_batch, yDataPH: y_batch})\n",
    "                    \n",
    "                    # build preconditioner \n",
    "                    if use_preconditioner:\n",
    "                        grad2=gradient*gradient*(1-beta1)+beta1*grad2\n",
    "                        preconditioner=1. /(np.sqrt(grad2)+lam)\n",
    "                        preconditioner = preconditioner / float(np.mean(preconditioner))\n",
    "                        \n",
    "                    gradient_times_velocity_variance=better_gradient_variance_calculation.eval( \\\n",
    "                        feed_dict={xDataPH: x_batch, yDataPH: y_batch, velocity_dummy: preconditioner*velocity_input})\n",
    "                                                                                               \n",
    "                # apply some operation to the gradient\n",
    "                new_gradient=my_updater.update(preconditioner*gradient,gradient_times_velocity_variance)\n",
    "            else:\n",
    "                with sess.as_default():\n",
    "                    gradient=g.eval(feed_dict={xDataPH: x_batch, yDataPH: y_batch})\n",
    "                new_gradient=my_updater.update(gradient)  \n",
    "            # apply gradient and track quantities\n",
    "            with sess.as_default():\n",
    "                # apply the new quantity to the parameters\n",
    "                train_step.run(feed_dict={ghostGradient: preconditioner*new_gradient})\n",
    "                \n",
    "                if my_updater.__class__ == lSBPS:\n",
    "                    my_updater.all_vs[-1] = preconditioner*my_updater.all_vs[-1]\n",
    "                    perrs.append(my_updater.p_errs)\n",
    "                # store samples\n",
    "                samples[iter,:] = weights.eval(feed_dict={xDataPH: x_batch, yDataPH: y_batch})\n",
    "        \n",
    "            if np.mod(iter,NLL_factor) == 0:\n",
    "                NLL_store[n_i]=sess.run(NLL, feed_dict={xDataPH: xData, yDataPH: yData})\n",
    "                #print 'Evaluating NLL - ', NLL_store[n_i]\n",
    "                n_i += 1\n",
    "            # update iteration counter\n",
    "            iter+=1\n",
    "    end = time.time()\n",
    "    print 'Runtime :', end - start\n",
    "    return NLL_store,acc_store,samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_SBPS_samples(c_SBPS,W_map,W,N):\n",
    "    # initialize tf for optimization\n",
    "    print 'Generating discrete samples and calculating NLL'\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    # how long to run optimization\n",
    "    time_step = c_SBPS.total_time/total_iter\n",
    "    j = 0\n",
    "    n_i = 0\n",
    "    NLL_store = np.zeros(total_iter/NLL_factor)\n",
    "    samples = np.zeros((total_iter,c_SBPS.D))\n",
    "    total_time_moved = 0\n",
    "    all_curr_ts = []\n",
    "    i = 0\n",
    "    time_until_next_sample = time_step\n",
    "    wset=W.assign(W_map)\n",
    "    with sess.as_default():\n",
    "        sess.run(wset)\n",
    "\n",
    "    while i < total_iter - 1:\n",
    "        curr_time = c_SBPS.all_times[j]\n",
    "        while curr_time > time_until_next_sample:\n",
    "            curr_time -= time_until_next_sample\n",
    "            g = time_until_next_sample*c_SBPS.all_vs[j]\n",
    "            with sess.as_default():\n",
    "                train_step.run(feed_dict={ghostGradient: g})\n",
    "                if i < total_iter:\n",
    "                    samples[i,:] = weights.eval(feed_dict={xDataPH: xData, yDataPH: yData})\n",
    "                if np.mod(i,NLL_factor) == 0 and i < total_iter:\n",
    "                    NLL_store[n_i]=sess.run(NLL, feed_dict={xDataPH: xData, yDataPH: yData})\n",
    "                    #print 'Evaluating NLL - ', NLL_store[n_i]\n",
    "                    n_i += 1\n",
    "            i += 1\n",
    "            time_until_next_sample = time_step\n",
    "\n",
    "        # moving remainder\n",
    "        time_until_next_sample -= curr_time\n",
    "        g = curr_time*c_SBPS.all_vs[j]\n",
    "        with sess.as_default():\n",
    "            train_step.run(feed_dict={ghostGradient: g})\n",
    "        j += 1\n",
    "    return NLL_store,samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 151.89334106    0.29898757   40.99404907   36.25157547   35.4541626\n",
      "   31.39330292   30.95247841   11.10289001   28.33082962   27.00761223\n",
      "   12.35878754   13.82955933   15.6628437    16.03650856   17.0235157\n",
      "   24.48535347   22.73886871   21.38258171   20.34235001   19.67732048]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Hessian\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "#if D < 10:\n",
    "Wdummy=tf.placeholder(tfbase,[D,1])\n",
    "Wset=W.assign(Wdummy)\n",
    "with sess.as_default():\n",
    "    sess.run(Wset,feed_dict={Wdummy: W_map})\n",
    "    hessian_map=hessian_calculation.eval(feed_dict={xDataPH: xData, yDataPH: yData})\n",
    "hessian_map=hessian_map*N\n",
    "cov_map=np.linalg.inv(hessian_map)\n",
    "print np.linalg.eig(hessian_map)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run samplers\n",
    "def run_sam():\n",
    "    n_epochs=100\n",
    "    batch_size=100\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    total_iter=n_epochs*(N/batch_size)\n",
    "    print('Running ' + str(n_epochs) + ' epochs (' +str(total_iter) + ' iterations) per method')\n",
    "    fW_map = np.ndarray.flatten(W_map)\n",
    "\n",
    "    W_start = np.random.multivariate_normal(fW_map,cov_map*10)\n",
    "    W_start = np.expand_dims(W_start,1)\n",
    "    k = 3\n",
    "    step_size = .1\n",
    "    labels = []\n",
    "    trajectories = []\n",
    "    NLLs = []\n",
    "    mbs = []\n",
    "\n",
    "    # lSBPS\n",
    "    k = 3\n",
    "    print('Running lSBPS, k = ' + str(k))\n",
    "    my_lSBPS = lSBPS(D,N,batch_size, [0,0],k)\n",
    "    my_lSBPS.reset_counter()\n",
    "    NLL_lSBPS_0,acc_lSBPS,samples_lSBPS_0 = runOptimizationExplicit(my_lSBPS,0,W_start)\n",
    "    NLL_lSBPS,samples_lSBPS = generate_SBPS_samples(my_lSBPS,W_start,W,N)\n",
    "    print('Number of bounces: ' + str(my_lSBPS.num_bounce))\n",
    "    num_pacc = (my_lSBPS.p_errs/float(my_lSBPS.num_bounce))\n",
    "    print('percent p(acc)>1: ', num_pacc)\n",
    "    reject = (len(samples_lSBPS[:,0]) - my_lSBPS.num_bounce)/float(len(samples_lSBPS[:,0]))\n",
    "    print('percent rejections: ', reject)\n",
    "    print('percent negative slope: ', my_lSBPS.neg_S/float(len(my_lSBPS.all_Gs)))\n",
    "    print 'total travel time: ', my_lSBPS.total_time \n",
    "    labels.append('SBPS')\n",
    "    trajectories.append(samples_lSBPS)\n",
    "    NLLs.append(NLL_lSBPS)\n",
    "    mbs.append(batch_size)\n",
    "\n",
    "    ## Run SGLD\n",
    "    stepsizes = [1,.1,.01]\n",
    "    for i in range(len(stepsizes)):\n",
    "        print('Running SGLD ', stepsizes )\n",
    "        total_iter=n_epochs*(N/batch_size)\n",
    "        my_SGLD = SGLD(N,stepsizes[i])\n",
    "        NLL_SGLD,acc_SGLD,samples_SGLD = runOptimizationExplicit(my_SGLD,0,W_start)\n",
    "        labels.append('SGLD ' + str(stepsizes[i]))\n",
    "        trajectories.append(samples_SGLD)\n",
    "        NLLs.append(NLL_SGLD)\n",
    "        mbs.append(batch_size)\n",
    "    '''\n",
    "    # Run mSGNHT\n",
    "    batch_size=100\n",
    "    total_iter=n_epochs*(N/batch_size)\n",
    "    my_mSGNHT = mSGNHT(D,N,.1)\n",
    "    print 'Running mSGNHT'\n",
    "    NLL_mSGNHT,acc_mSGNHT,samples_mSGNHT = runOptimizationExplicit(my_mSGNHT,0,W_start)\n",
    "    labels.append('mSGNHT')\n",
    "    trajectories.append(samples_mSGNHT)\n",
    "    NLLs.append(NLL_mSGNHT)\n",
    "    mbs.append(batch_size)\n",
    "\n",
    "    # lipschitz BPS\n",
    "    print 'Running lipSBPS'\n",
    "    batch_size=1\n",
    "    total_iter=n_epochs*(N/batch_size)\n",
    "    verbose=False\n",
    "    L=np.sqrt(D)*np.max(np.abs(xData)) # this could be tightened, probably by a factor of 2\n",
    "    my_bouncer = lipschitzSBPS(D,N,L,verbose)\n",
    "    NLL_lipSBPS0,acc_lipSBPS,samples_lipSBPS0 = runOptimizationExplicit(my_bouncer,0,W_start)\n",
    "    NLL_lipSBPS0 = 0\n",
    "    acc_lipSBPS = 0\n",
    "    my_bouncer.all_vs = [-v for v in my_bouncer.all_vs]\n",
    "    NLL_lipSBPS,samples_lipSBPS = generate_SBPS_samples(my_bouncer,W_start,W,N)\n",
    "    labels.append('lipSBPS')\n",
    "    trajectories.append(samples_lipSBPS)\n",
    "    NLLs.append(NLL_lipSBPS)\n",
    "    mbs.append(batch_size)\n",
    "\n",
    "    # Run ZZ-SS\n",
    "    batch_size=1\n",
    "    total_iter=n_epochs*(N/batch_size)\n",
    "    logisticRegressionM=np.max(xData)\n",
    "    my_zigzag_ss=ZZ_SS(D,N,logisticRegressionM)\n",
    "    my_zigzag_ss.reset_counter()\n",
    "    print('Running ZZ-SS')\n",
    "    NLL_zz_ss0,acc_zz_ss,samples_zz0 = runOptimizationExplicit(my_zigzag_ss,0,W_start)\n",
    "    acc_zz_ss = 0\n",
    "    NLL_zz_ss0 = 0\n",
    "    print('Number of flips: ' + str(my_zigzag_ss.flipcounter))\n",
    "    NLL_zz_ss,samples_zz = generate_SBPS_samples(my_zigzag_ss,W_start,W,N)\n",
    "    labels.append('ZZ-SS')\n",
    "    trajectories.append(samples_zz)\n",
    "    NLLs.append(NLL_zz_ss)\n",
    "    mbs.append(1)\n",
    "    '''\n",
    "    wset=W.assign(W_map)\n",
    "    with sess.as_default():\n",
    "        sess.run(wset)\n",
    "    print 'MAP assigned, x coordinate is  - ', W_start[0]    \n",
    "    NLL_map = sess.run(NLL, feed_dict={xDataPH: xData, yDataPH: yData})\n",
    "    cond_num = np.max(np.linalg.eig(hessian_map)[0])/float(np.min(np.linalg.eig(hessian_map)[0]))\n",
    "    print 'condition number ', cond_num\n",
    "    stats = [total_iter,cond_num,my_lSBPS.num_bounce,my_lSBPS.p_errs/float(my_lSBPS.num_bounce),\\\n",
    "            (len(samples_lSBPS[:,0]) - my_lSBPS.num_bounce)/float(len(samples_lSBPS[:,0])),\\\n",
    "            my_lSBPS.neg_S/float(len(my_lSBPS.all_Gs)),my_lSBPS.total_time]\n",
    "            # my_plSBPS.num_bounce,my_plSBPS.p_errs/float(my_plSBPS.num_bounce),\\\n",
    "            #(len(samples_plSBPS[:,0]) - my_plSBPS.num_bounce)/float(len(samples_plSBPS[:,0])),\\\n",
    "            #my_plSBPS.neg_S/float(len(my_plSBPS.all_Gs)),my_plSBPS.total_time]\n",
    "    #np.save('samples_' + str(D),[stats,trajectories,NLLs,labels,NLL_map])\n",
    "    #print 'saved samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 epochs (1000 iterations) per method\n",
      "Running lSBPS, k = 3\n",
      "Runtime : 34.5081648827\n",
      "Generating discrete samples and calculating NLL\n",
      "Number of bounces: 146\n",
      "('percent p(acc)>1: ', 0.06164383561643835)\n",
      "('percent rejections: ', 0.854)\n",
      "('percent negative slope: ', 0.393)\n",
      "total travel time:  19.0200820379\n",
      "('Running SGLD ', [1, 0.1, 0.01])\n",
      "Runtime : 20.9941210747\n",
      "('Running SGLD ', [1, 0.1, 0.01])\n",
      "Runtime : 22.2493648529\n",
      "('Running SGLD ', [1, 0.1, 0.01])\n",
      "Runtime : 21.8827528954\n",
      "MAP assigned, x coordinate is  -  [ 2.05440832]\n",
      "condition number  508.025609134\n",
      "         4260479 function calls (4220891 primitive calls) in 58.908 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        6    0.000    0.000    0.000    0.000 :0(TF_CloseSession)\n",
      "    17009    0.119    0.000    0.119    0.000 :0(TF_DeleteBuffer)\n",
      "        6    0.012    0.002    0.012    0.002 :0(TF_DeleteSession)\n",
      "        6    0.000    0.000    0.000    0.000 :0(TF_DeleteSessionOptions)\n",
      "    17039    0.112    0.000    0.112    0.000 :0(TF_DeleteStatus)\n",
      "       12    0.022    0.002    0.022    0.002 :0(TF_ExtendGraph)\n",
      "    17039    0.129    0.000    0.129    0.000 :0(TF_GetCode)\n",
      "    17009    0.133    0.000    0.133    0.000 :0(TF_NewBuffer)\n",
      "        6    0.000    0.000    0.000    0.000 :0(TF_NewSession)\n",
      "    17039    0.098    0.000    0.098    0.000 :0(TF_NewStatus)\n",
      "    17009   11.030    0.001   11.030    0.001 :0(TF_Run)\n",
      "        6    0.001    0.000    0.001    0.000 :0(_TF_NewSessionOptions)\n",
      "        6    0.000    0.000    0.000    0.000 :0(_TF_SetTarget)\n",
      "     1160    0.005    0.000    0.005    0.000 :0(__array_prepare__)\n",
      "        6    0.000    0.000    0.000    0.000 :0(__new__)\n",
      "        6    0.000    0.000    0.000    0.000 :0(__reduce_ex__)\n",
      "       12    0.000    0.000    0.000    0.000 :0(add)\n",
      "       23    0.000    0.000    0.002    0.000 :0(all)\n",
      "       12    0.000    0.000    0.000    0.000 :0(allocate_lock)\n",
      "        5    0.000    0.000    0.000    0.000 :0(any)\n",
      "    80999    0.481    0.000    0.481    0.000 :0(append)\n",
      "      400    0.006    0.000    0.006    0.000 :0(arange)\n",
      "    49794    0.600    0.000    0.600    0.000 :0(array)\n",
      "     1169    0.008    0.000    0.008    0.000 :0(astype)\n",
      "        6    0.000    0.000    0.000    0.000 :0(callable)\n",
      "    15533    0.078    0.000    0.078    0.000 :0(chr)\n",
      "        3    0.000    0.000    0.000    0.000 :0(compress)\n",
      "      108    0.001    0.000    0.003    0.000 :0(decode)\n",
      "     9358    0.100    0.000    0.100    0.000 :0(dot)\n",
      "    61576    0.405    0.000    0.405    0.000 :0(encode)\n",
      "       18    0.000    0.000    0.000    0.000 :0(exc_info)\n",
      "     2471    0.016    0.000    0.016    0.000 :0(extend)\n",
      "   179459    0.803    0.000    0.803    0.000 :0(get)\n",
      "        7    0.000    0.000    0.000    0.000 :0(get_ident)\n",
      "    49242    0.312    0.000    0.312    0.000 :0(getattr)\n",
      "       16    0.000    0.000    0.000    0.000 :0(geterrobj)\n",
      "       48    0.000    0.000    0.000    0.000 :0(getpid)\n",
      "       12    0.000    0.000    0.000    0.000 :0(getvalue)\n",
      "     1345    0.007    0.000    0.007    0.000 :0(hasattr)\n",
      "    58054    0.238    0.000    0.238    0.000 :0(id)\n",
      "        1    0.000    0.000    0.000    0.000 :0(index)\n",
      "   730024    3.819    0.000    3.819    0.000 :0(isinstance)\n",
      "     4862    0.023    0.000    0.023    0.000 :0(issubclass)\n",
      "    77279    0.518    0.000    0.518    0.000 :0(items)\n",
      "    32699    0.152    0.000    0.152    0.000 :0(iter)\n",
      "       18    0.000    0.000    0.000    0.000 :0(iteritems)\n",
      "121354/119597    0.557    0.000    0.574    0.000 :0(len)\n",
      "       18    0.000    0.000    0.000    0.000 :0(match)\n",
      "     2554    0.014    0.000    0.014    0.000 :0(max)\n",
      "     1163    0.005    0.000    0.005    0.000 :0(min)\n",
      "        1    0.001    0.001    0.002    0.002 :0(multivariate_normal)\n",
      "       36    0.000    0.000    0.000    0.000 :0(pack)\n",
      "    10079    0.063    0.000    0.063    0.000 :0(pop)\n",
      "    15629    0.083    0.000    0.083    0.000 :0(proxy)\n",
      "     3000    0.019    0.000    0.019    0.000 :0(rand)\n",
      "     3000    0.049    0.000    0.049    0.000 :0(randn)\n",
      "     2893    0.016    0.000    0.016    0.000 :0(range)\n",
      "    14820    0.070    0.000    0.070    0.000 :0(ravel)\n",
      "     3668    0.051    0.000    0.051    0.000 :0(reduce)\n",
      "       13    0.000    0.000    0.000    0.000 :0(reshape)\n",
      "       18    0.000    0.000    0.000    0.000 :0(reverse)\n",
      "        5    0.000    0.000    0.000    0.000 :0(rstrip)\n",
      "     4026    0.048    0.000    0.210    0.000 :0(setattr)\n",
      "     6132    0.033    0.000    0.033    0.000 :0(setdefault)\n",
      "        8    0.000    0.000    0.000    0.000 :0(seterrobj)\n",
      "        1    0.000    0.000    0.000    0.000 :0(setprofile)\n",
      "      400    0.046    0.000    0.057    0.000 :0(shuffle)\n",
      "    17389    0.263    0.000    0.424    0.000 :0(sort)\n",
      "       42    0.001    0.000    0.001    0.000 :0(sorted)\n",
      "       24    0.000    0.000    0.000    0.000 :0(split)\n",
      "       12    0.000    0.000    0.000    0.000 :0(startswith)\n",
      "        8    0.000    0.000    0.000    0.000 :0(time)\n",
      "        6    0.000    0.000    0.000    0.000 :0(tostring)\n",
      "    17015    0.100    0.000    0.100    0.000 :0(update)\n",
      "       24    0.000    0.000    0.000    0.000 :0(upper)\n",
      "      108    0.001    0.000    0.001    0.000 :0(utf_8_decode)\n",
      "       12    0.000    0.000    0.000    0.000 :0(values)\n",
      "    38066    0.201    0.000    0.201    0.000 :0(write)\n",
      "     1177    0.010    0.000    0.010    0.000 :0(zeros)\n",
      "    46048    0.326    0.000    0.326    0.000 :0(zip)\n",
      "        1    0.143    0.143    8.286    8.286 <ipython-input-10-518b866db4c5>:1(generate_SBPS_samples)\n",
      "        1    0.001    0.001   58.905   58.905 <ipython-input-15-bb880caac908>:2(run_sam)\n",
      "        4    0.692    0.173   49.313   12.328 <ipython-input-9-9abc3cccaea8>:1(runOptimizationExplicit)\n",
      "        1    0.001    0.001   58.908   58.908 <string>:1(<module>)\n",
      "      800    0.006    0.000    0.006    0.000 _internal.py:227(__init__)\n",
      "      800    0.004    0.000    0.004    0.000 _internal.py:252(get_data)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:25(_amax)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:28(_amin)\n",
      "     2317    0.021    0.000    0.053    0.000 _methods.py:31(_sum)\n",
      "        5    0.000    0.000    0.000    0.000 _methods.py:37(_any)\n",
      "        5    0.000    0.000    0.000    0.000 _methods.py:40(_all)\n",
      "     1332    0.023    0.000    0.035    0.000 _methods.py:43(_count_reduce_items)\n",
      "     1332    0.053    0.000    0.179    0.000 _methods.py:53(_mean)\n",
      "        1    0.000    0.000    0.000    0.000 _util.py:141(_asarray_validated)\n",
      "        6    0.000    0.000    0.000    0.000 _weakrefset.py:70(__contains__)\n",
      "        6    0.000    0.000    0.000    0.000 abc.py:128(__instancecheck__)\n",
      "        1    0.000    0.000    0.004    0.004 arrayprint.py:237(_array2string)\n",
      "        1    0.000    0.000    0.004    0.004 arrayprint.py:340(array2string)\n",
      "        1    0.000    0.000    0.000    0.000 arrayprint.py:450(_extendLine)\n",
      "        1    0.000    0.000    0.001    0.001 arrayprint.py:458(_formatArray)\n",
      "        3    0.000    0.000    0.002    0.001 arrayprint.py:529(__init__)\n",
      "        3    0.001    0.000    0.002    0.001 arrayprint.py:543(fillFormat)\n",
      "        1    0.000    0.000    0.000    0.000 arrayprint.py:594(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 arrayprint.py:628(_digits)\n",
      "        1    0.000    0.000    0.000    0.000 arrayprint.py:635(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 arrayprint.py:657(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 arrayprint.py:685(__init__)\n",
      "        1    0.000    0.000    0.001    0.001 arrayprint.py:696(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 arrayprint.py:713(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 arrayprint.py:734(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:887(isspmatrix)\n",
      "        1    0.000    0.000    0.000    0.000 blas.py:177(find_best_blas_type)\n",
      "        1    0.000    0.000    0.000    0.000 blas.py:223(_get_funcs)\n",
      "        6    0.000    0.000    0.000    0.000 common_shapes.py:523(no_outputs)\n",
      "    46072    0.601    0.000    1.081    0.000 compat.py:27(as_bytes)\n",
      "        6    0.001    0.000    0.068    0.011 constant_op.py:116(constant)\n",
      "        6    0.000    0.000    0.004    0.001 constant_op.py:170(_ConstantShape)\n",
      "        6    0.000    0.000    0.068    0.011 constant_op.py:176(_constant_tensor_conversion_function)\n",
      "     3392    0.021    0.000    0.021    0.000 containers.py:192(__init__)\n",
      "    12045    0.066    0.000    0.066    0.000 containers.py:202(__getitem__)\n",
      "     6501    0.064    0.000    0.094    0.000 containers.py:206(__len__)\n",
      "     2447    0.034    0.000    0.049    0.000 containers.py:237(__init__)\n",
      "       30    0.001    0.000    0.002    0.000 containers.py:261(extend)\n",
      "     2423    0.044    0.000    0.073    0.000 containers.py:280(MergeFrom)\n",
      "       18    0.000    0.000    0.000    0.000 containers.py:325(__delslice__)\n",
      "      945    0.012    0.000    0.018    0.000 containers.py:350(__init__)\n",
      "       12    0.000    0.000    0.001    0.000 containers.py:368(add)\n",
      "2315/2018    0.091    0.000    1.443    0.001 containers.py:379(extend)\n",
      "      927    0.012    0.000    0.116    0.000 containers.py:393(MergeFrom)\n",
      "     1230    0.009    0.000    0.009    0.000 containers.py:525(__init__)\n",
      "     8046    0.122    0.000    0.381    0.000 containers.py:541(__getitem__)\n",
      "       18    0.000    0.000    0.000    0.000 containers.py:576(__contains__)\n",
      "     2436    0.024    0.000    0.036    0.000 containers.py:586(__len__)\n",
      "     3654    0.039    0.000    0.059    0.000 containers.py:589(__iter__)\n",
      "     1218    0.065    0.000    0.900    0.001 containers.py:595(MergeFrom)\n",
      "    27126    0.130    0.000    0.130    0.000 contextlib.py:12(__init__)\n",
      "27114/27090    0.264    0.000    0.676    0.000 contextlib.py:15(__enter__)\n",
      "27114/27090    0.647    0.000    1.480    0.000 contextlib.py:21(__exit__)\n",
      "    27126    0.298    0.000    0.428    0.000 contextlib.py:82(helper)\n",
      "        6    0.000    0.000    0.030    0.005 control_flow_ops.py:1961(_GroupControlDeps)\n",
      "        6    0.000    0.000    0.033    0.006 control_flow_ops.py:1971(group)\n",
      "        1    0.000    0.000    0.000    0.000 copy.py:101(_copy_immutable)\n",
      "       18    0.001    0.000    0.019    0.001 copy.py:145(deepcopy)\n",
      "       18    0.000    0.000    0.001    0.000 copy.py:267(_keep_alive)\n",
      "        6    0.000    0.000    0.001    0.000 copy.py:306(_reconstruct)\n",
      "     2006    0.043    0.000    0.066    0.000 copy.py:66(copy)\n",
      "        6    0.000    0.000    0.000    0.000 copy_reg.py:92(__newobj__)\n",
      "        1    0.000    0.000    0.000    0.000 core.py:6042(isMaskedArray)\n",
      "        1    0.000    0.000    0.001    0.001 decomp_svd.py:15(svd)\n",
      "     1206    0.007    0.000    0.007    0.000 descriptor.py:118(GetOptions)\n",
      "       12    0.000    0.000    0.000    0.000 device.py:105(replica)\n",
      "       12    0.000    0.000    0.000    0.000 device.py:109(replica)\n",
      "       12    0.000    0.000    0.000    0.000 device.py:116(task)\n",
      "       12    0.000    0.000    0.000    0.000 device.py:120(task)\n",
      "       12    0.001    0.000    0.001    0.000 device.py:127(parse_from_string)\n",
      "        6    0.000    0.000    0.000    0.000 device.py:174(merge_from)\n",
      "        6    0.000    0.000    0.000    0.000 device.py:191(to_string)\n",
      "       12    0.000    0.000    0.002    0.000 device.py:212(from_string)\n",
      "        6    0.000    0.000    0.001    0.000 device.py:254(merge_device)\n",
      "        6    0.000    0.000    0.003    0.001 device.py:282(_device_function)\n",
      "       12    0.000    0.000    0.001    0.000 device.py:64(__init__)\n",
      "       12    0.000    0.000    0.000    0.000 device.py:87(_clear)\n",
      "       12    0.000    0.000    0.000    0.000 device.py:94(job)\n",
      "       12    0.000    0.000    0.000    0.000 device.py:98(job)\n",
      "       54    0.000    0.000    0.000    0.000 dtypes.py:100(is_ref_dtype)\n",
      "        6    0.000    0.000    0.000    0.000 dtypes.py:105(as_ref)\n",
      "       42    0.000    0.000    0.001    0.000 dtypes.py:113(base_dtype)\n",
      "    29003    0.133    0.000    0.133    0.000 dtypes.py:132(as_numpy_dtype)\n",
      "       90    0.000    0.000    0.000    0.000 dtypes.py:137(as_datatype_enum)\n",
      "       18    0.001    0.000    0.001    0.000 dtypes.py:222(is_compatible_with)\n",
      "       42    0.001    0.000    0.001    0.000 dtypes.py:245(__eq__)\n",
      "        6    0.000    0.000    0.000    0.000 dtypes.py:250(__ne__)\n",
      "        6    0.000    0.000    0.000    0.000 dtypes.py:265(__hash__)\n",
      "      102    0.001    0.000    0.002    0.000 dtypes.py:499(as_dtype)\n",
      "      300    0.009    0.000    0.015    0.000 encoder.py:134(PackedFieldSize)\n",
      "     3414    0.035    0.000    0.053    0.000 encoder.py:148(FieldSize)\n",
      "       96    0.002    0.000    0.004    0.000 encoder.py:192(PackedFieldSize)\n",
      "      540    0.003    0.000    0.003    0.000 encoder.py:203(FieldSize)\n",
      "     1103    0.066    0.000    0.146    0.000 encoder.py:238(RepeatedFieldSize)\n",
      "     6850    0.146    0.000    0.266    0.000 encoder.py:246(FieldSize)\n",
      "      288    0.011    0.000    0.024    0.000 encoder.py:260(RepeatedFieldSize)\n",
      "      162    0.003    0.000    0.004    0.000 encoder.py:268(FieldSize)\n",
      "      222    0.010    0.000    0.048    0.000 encoder.py:299(RepeatedFieldSize)\n",
      "7464/4002    0.124    0.000    1.028    0.000 encoder.py:307(FieldSize)\n",
      "     1230    0.062    0.000    1.759    0.001 encoder.py:351(FieldSize)\n",
      "    13307    0.216    0.000    0.356    0.000 encoder.py:372(EncodeVarint)\n",
      "     1833    0.030    0.000    0.048    0.000 encoder.py:388(EncodeSignedVarint)\n",
      "      150    0.008    0.000    0.020    0.000 encoder.py:440(EncodePackedField)\n",
      "     1683    0.026    0.000    0.079    0.000 encoder.py:458(EncodeField)\n",
      "       36    0.002    0.000    0.005    0.000 encoder.py:581(EncodePackedField)\n",
      "       12    0.001    0.000    0.002    0.000 encoder.py:650(EncodePackedField)\n",
      "      258    0.004    0.000    0.007    0.000 encoder.py:671(EncodeField)\n",
      "     1091    0.079    0.000    0.192    0.000 encoder.py:687(EncodeRepeatedField)\n",
      "     4744    0.155    0.000    0.387    0.000 encoder.py:695(EncodeField)\n",
      "      141    0.005    0.000    0.012    0.000 encoder.py:711(EncodeRepeatedField)\n",
      "       75    0.002    0.000    0.005    0.000 encoder.py:718(EncodeField)\n",
      "   117/12    0.048    0.000    5.646    0.470 encoder.py:753(EncodeRepeatedField)\n",
      "4647/1980    0.132    0.000    1.627    0.001 encoder.py:760(EncodeField)\n",
      "     1206    0.059    0.000    2.645    0.002 encoder.py:818(EncodeField)\n",
      "    17466    0.088    0.000    0.088    0.000 encoder.py:82(_VarintSize)\n",
      "     3864    0.020    0.000    0.020    0.000 encoder.py:96(_SignedVarintSize)\n",
      "    34078    0.564    0.000    0.903    0.000 errors.py:441(raise_exception_on_not_ok_status)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1383(ravel)\n",
      "     2317    0.039    0.000    0.105    0.000 fromnumeric.py:1743(sum)\n",
      "        5    0.000    0.000    0.001    0.000 fromnumeric.py:1892(any)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1973(all)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2180(amax)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2271(amin)\n",
      "     1332    0.019    0.000    0.198    0.000 fromnumeric.py:2796(mean)\n",
      "        1    0.000    0.000    0.000    0.000 function_base.py:970(asarray_chkfinite)\n",
      "        6    0.000    0.000    0.025    0.004 gen_control_flow_ops.py:177(no_op)\n",
      "        6    0.000    0.000    0.150    0.025 gen_state_ops.py:19(assign)\n",
      "        1    0.000    0.000    0.000    0.000 getlimits.py:245(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 getlimits.py:270(max)\n",
      "        7    0.000    0.000    0.001    0.000 ioloop.py:928(add_callback)\n",
      "       48    0.001    0.000    0.001    0.000 iostream.py:227(_is_master_process)\n",
      "       48    0.001    0.000    0.002    0.000 iostream.py:240(_schedule_flush)\n",
      "       48    0.002    0.000    0.007    0.000 iostream.py:308(write)\n",
      "        1    0.000    0.000    0.000    0.000 lapack.py:405(get_lapack_funcs)\n",
      "        1    0.000    0.000    0.000    0.000 lapack.py:447(_compute_lwork)\n",
      "        2    0.001    0.000    0.001    0.001 linalg.py:1000(eig)\n",
      "     1160    0.008    0.000    0.008    0.000 linalg.py:101(get_linalg_error_extobj)\n",
      "     1160    0.017    0.000    0.038    0.000 linalg.py:106(_makearray)\n",
      "     2335    0.019    0.000    0.029    0.000 linalg.py:111(isComplexType)\n",
      "     1162    0.010    0.000    0.015    0.000 linalg.py:124(_realType)\n",
      "     1160    0.028    0.000    0.063    0.000 linalg.py:139(_commonType)\n",
      "       13    0.001    0.000    0.001    0.000 linalg.py:1976(norm)\n",
      "     1160    0.012    0.000    0.016    0.000 linalg.py:198(_assertRankAtLeast2)\n",
      "     1160    0.017    0.000    0.027    0.000 linalg.py:209(_assertNdSquareness)\n",
      "        2    0.000    0.000    0.000    0.000 linalg.py:214(_assertFinite)\n",
      "     1158    0.100    0.000    0.280    0.000 linalg.py:458(inv)\n",
      "       60    0.001    0.000    0.017    0.000 message.py:106(CopyFrom)\n",
      "       18    0.000    0.000    0.017    0.001 message.py:69(__deepcopy__)\n",
      "       72    0.000    0.000    0.000    0.000 message_listener.py:77(Modified)\n",
      "        1    0.000    0.000    0.000    0.000 misc.py:169(_datacopied)\n",
      "     7403    0.256    0.000    0.562    0.000 numeric.py:1015(outer)\n",
      "        1    0.000    0.000    0.004    0.004 numeric.py:1835(array_str)\n",
      "        8    0.000    0.000    0.001    0.000 numeric.py:2576(seterr)\n",
      "        8    0.000    0.000    0.000    0.000 numeric.py:2676(geterr)\n",
      "        4    0.000    0.000    0.000    0.000 numeric.py:2963(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 numeric.py:2967(__enter__)\n",
      "        4    0.000    0.000    0.000    0.000 numeric.py:2972(__exit__)\n",
      "    19456    0.173    0.000    0.315    0.000 numeric.py:414(asarray)\n",
      "     1340    0.012    0.000    0.052    0.000 numeric.py:484(asanyarray)\n",
      "        2    0.000    0.000    0.000    0.000 numerictypes.py:942(_can_coerce_all)\n",
      "        1    0.000    0.000    0.000    0.000 numerictypes.py:964(find_common_type)\n",
      "        6    0.000    0.000    0.000    0.000 op_def_library.py:166(_MakeBool)\n",
      "        6    0.000    0.000    0.000    0.000 op_def_library.py:173(_MakeType)\n",
      "    36/24    0.000    0.000    0.002    0.000 op_def_library.py:244(_MaybeColocateWith)\n",
      "       12    0.007    0.001    0.175    0.015 op_def_library.py:289(apply_op)\n",
      "        6    0.000    0.000    0.000    0.000 op_def_library.py:39(_Attr)\n",
      "        6    0.000    0.000    0.000    0.000 op_def_library.py:47(_AttrValue)\n",
      "       12    0.000    0.000    0.001    0.000 op_def_library.py:54(_SatisfiesTypeConstraint)\n",
      "       12    0.000    0.000    0.000    0.000 op_def_library.py:558(<genexpr>)\n",
      "       12    0.000    0.000    0.000    0.000 op_def_library.py:64(_IsListParameter)\n",
      "       24    0.000    0.000    0.000    0.000 op_def_library.py:80(_IsListValue)\n",
      "       12    0.000    0.000    0.001    0.000 op_def_library.py:84(_Flatten)\n",
      "        6    0.000    0.000    0.000    0.000 op_def_library.py:92(_Restructure)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:1065(_device_string)\n",
      "       18    0.001    0.000    0.019    0.001 ops.py:1072(_NodeDef)\n",
      "       18    0.003    0.000    0.070    0.004 ops.py:1139(__init__)\n",
      "       24    0.000    0.000    0.001    0.000 ops.py:1209(<genexpr>)\n",
      "    46030    0.418    0.000    0.713    0.000 ops.py:123(_as_graph_element)\n",
      "        6    0.000    0.000    0.001    0.000 ops.py:1243(colocation_groups)\n",
      "   115072    1.060    0.000    2.559    0.000 ops.py:1280(name)\n",
      "       36    0.000    0.000    0.000    0.000 ops.py:1285(_id)\n",
      "       18    0.000    0.000    0.001    0.000 ops.py:1290(device)\n",
      "        6    0.000    0.000    0.001    0.000 ops.py:1301(_set_device)\n",
      "       18    0.001    0.000    0.005    0.000 ops.py:1403(_recompute_node_def)\n",
      "     1436    0.009    0.000    0.009    0.000 ops.py:1413(outputs)\n",
      "       12    0.000    0.000    0.000    0.000 ops.py:1422(__init__)\n",
      "       12    0.000    0.000    0.000    0.000 ops.py:1437(__getitem__)\n",
      "       12    0.000    0.000    0.000    0.000 ops.py:1441(inputs)\n",
      "    11024    0.106    0.000    0.252    0.000 ops.py:1466(type)\n",
      "    62020    0.267    0.000    0.267    0.000 ops.py:1471(graph)\n",
      "     2782    0.019    0.000    0.019    0.000 ops.py:1476(node_def)\n",
      "       12    0.001    0.000    0.004    0.000 ops.py:1503(get_attr)\n",
      "     5997    0.090    0.000   10.191    0.002 ops.py:1534(run)\n",
      "       18    0.001    0.000    0.014    0.001 ops.py:1692(set_shapes_for_outputs)\n",
      "       54    0.000    0.000    0.000    0.000 ops.py:1987(_check_not_finalized)\n",
      "       18    0.001    0.000    0.002    0.000 ops.py:1996(_add_op)\n",
      "    34030    0.149    0.000    0.149    0.000 ops.py:2019(version)\n",
      "       18    0.000    0.000    0.000    0.000 ops.py:2064(_get_control_flow_context)\n",
      "       12    0.068    0.006    1.552    0.129 ops.py:2080(as_graph_def)\n",
      "       18    0.002    0.000    0.122    0.007 ops.py:2184(create_op)\n",
      "    46012    0.615    0.000    3.708    0.000 ops.py:2292(as_graph_element)\n",
      "    46012    1.303    0.000    3.094    0.000 ops.py:2324(_as_graph_element_locked)\n",
      "       18    0.000    0.000    0.000    0.000 ops.py:2470(_next_id)\n",
      "       24    0.000    0.000    0.001    0.000 ops.py:2481(as_default)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2580(get_collection)\n",
      "       36    0.000    0.000    0.001    0.000 ops.py:2647(name_scope)\n",
      "       18    0.000    0.000    0.000    0.000 ops.py:2736(unique_name)\n",
      "       12    0.000    0.000    0.001    0.000 ops.py:274(__init__)\n",
      "       12    0.000    0.000    0.001    0.000 ops.py:2781(colocate_with)\n",
      "       12    0.000    0.000    0.002    0.000 ops.py:2847(device)\n",
      "       18    0.000    0.000    0.004    0.000 ops.py:2917(_apply_device_functions)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2931(__init__)\n",
      "    51027    0.216    0.000    0.216    0.000 ops.py:296(op)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2965(__enter__)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2975(__exit__)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2982(control_inputs)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2986(add_op)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2992(_push_control_dependencies_controller)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2995(_pop_control_dependencies_controller)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:2999(_current_control_dependencies)\n",
      "       18    0.000    0.000    0.000    0.000 ops.py:3006(_control_dependencies_for_inputs)\n",
      "    29027    0.123    0.000    0.123    0.000 ops.py:301(dtype)\n",
      "       18    0.000    0.000    0.000    0.000 ops.py:3041(_record_op_seen_by_control_dependencies)\n",
      "        6    0.000    0.000    0.001    0.000 ops.py:3050(control_dependencies)\n",
      "    50008    0.454    0.000    0.670    0.000 ops.py:306(graph)\n",
      "    40003    0.825    0.000    2.600    0.000 ops.py:311(name)\n",
      "    28997    0.273    0.000    0.618    0.000 ops.py:3271(is_feedable)\n",
      "    17009    0.161    0.000    0.237    0.000 ops.py:3279(is_fetchable)\n",
      "        6    0.000    0.000    0.001    0.000 ops.py:3287(device)\n",
      "    29009    0.123    0.000    0.123    0.000 ops.py:329(get_shape)\n",
      "        6    0.000    0.000    0.000    0.000 ops.py:3305(colocate_with)\n",
      "       12    0.000    0.000    0.001    0.000 ops.py:3309(name_scope)\n",
      "        6    0.000    0.000    0.002    0.000 ops.py:3326(control_dependencies)\n",
      "    16050    0.153    0.000    0.220    0.000 ops.py:3353(get_default)\n",
      "    20054    0.215    0.000    0.337    0.000 ops.py:3367(get_controller)\n",
      "    10009    0.123    0.000    0.290    0.000 ops.py:3386(default_session)\n",
      "    15996    0.175    0.000    0.394    0.000 ops.py:3434(get_default_session)\n",
      "     9999    0.236    0.000   32.818    0.003 ops.py:3461(_eval_using_default_session)\n",
      "     5997    0.133    0.000   10.076    0.002 ops.py:3501(_run_using_default_session)\n",
      "       54    0.001    0.000    0.002    0.000 ops.py:3543(get_default)\n",
      "       12    0.000    0.000    0.000    0.000 ops.py:3550(_GetGlobalDefaultGraph)\n",
      "       54    0.001    0.000    0.002    0.000 ops.py:3578(get_default_graph)\n",
      "       18    0.001    0.000    0.001    0.000 ops.py:3611(_get_graph_from_inputs)\n",
      "       12    0.000    0.000    0.000    0.000 ops.py:373(set_shape)\n",
      "        6    0.000    0.000    0.001    0.000 ops.py:3802(get_collection)\n",
      "       12    0.000    0.000    0.002    0.000 ops.py:3832(op_scope)\n",
      "       12    0.000    0.000    0.000    0.000 ops.py:419(_add_consumer)\n",
      "       12    0.000    0.000    0.001    0.000 ops.py:432(_as_node_def_input)\n",
      "    57994    0.488    0.000    0.726    0.000 ops.py:463(__hash__)\n",
      "     9999    0.159    0.000   33.118    0.003 ops.py:533(eval)\n",
      "       18    0.000    0.000    0.000    0.000 ops.py:558(_TensorTensorConversionFunction)\n",
      "       24    0.001    0.000    0.071    0.003 ops.py:572(convert_to_tensor)\n",
      "        6    0.000    0.000    0.001    0.000 ops.py:639(convert_n_to_tensor)\n",
      "        6    0.000    0.000    0.001    0.000 ops.py:668(convert_to_tensor_or_indexed_slices)\n",
      "       18    0.005    0.000    0.009    0.000 ops.py:92(_extract_stack)\n",
      "        7    0.000    0.000    0.000    0.000 posix.py:53(wake)\n",
      "        0    0.000             0.000          profile:0(profiler)\n",
      "        1    0.000    0.000   58.908   58.908 profile:0(run_sam())\n",
      "    10151    0.063    0.000    0.063    0.000 python_message.py:1004(SetListener)\n",
      "15397/7609    0.321    0.000    2.928    0.000 python_message.py:1035(ByteSize)\n",
      "       12    0.000    0.000    5.750    0.479 python_message.py:1057(SerializeToString)\n",
      "       12    0.000    0.000    5.651    0.471 python_message.py:1071(SerializePartialToString)\n",
      "  6197/12    0.166    0.000    5.650    0.471 python_message.py:1077(InternalSerialize)\n",
      "  1412/12    0.057    0.000    0.099    0.008 python_message.py:1141(IsInitialized)\n",
      "10175/5492    0.479    0.000    2.026    0.000 python_message.py:1233(MergeFrom)\n",
      "       12    0.000    0.000    0.001    0.000 python_message.py:1274(WhichOneof)\n",
      "15677/10283    0.154    0.000    0.289    0.000 python_message.py:1317(Modified)\n",
      "     6072    0.071    0.000    0.104    0.000 python_message.py:1332(_UpdateOneofState)\n",
      "    15629    0.265    0.000    0.436    0.000 python_message.py:1362(__init__)\n",
      "    16971    0.124    0.000    0.152    0.000 python_message.py:1381(Modified)\n",
      "     1362    0.018    0.000    0.056    0.000 python_message.py:1397(__init__)\n",
      "     1362    0.025    0.000    0.072    0.000 python_message.py:1406(Modified)\n",
      "        6    0.000    0.000    0.000    0.000 python_message.py:271(_IsMapField)\n",
      "     1230    0.018    0.000    0.026    0.000 python_message.py:379(MakeMessageMapDefault)\n",
      "      945    0.012    0.000    0.030    0.000 python_message.py:415(MakeRepeatedMessageDefault)\n",
      "     2447    0.032    0.000    0.082    0.000 python_message.py:421(MakeRepeatedScalarDefault)\n",
      "     6195    0.130    0.000    0.647    0.000 python_message.py:429(MakeSubMessageDefault)\n",
      "       12    0.000    0.000    0.000    0.000 python_message.py:459(_GetIntegerEnumValue)\n",
      "14267/8477    0.606    0.000    2.283    0.000 python_message.py:474(init)\n",
      "     8052    0.044    0.000    0.044    0.000 python_message.py:537(_GetFieldByName)\n",
      "     1568    0.021    0.000    0.032    0.000 python_message.py:606(getter)\n",
      "   127782    1.137    0.000    1.671    0.000 python_message.py:651(getter)\n",
      "     4086    0.051    0.000    0.166    0.000 python_message.py:660(field_setter)\n",
      "       18    0.000    0.000    0.001    0.000 python_message.py:675(setter)\n",
      "       54    0.001    0.000    0.003    0.000 python_message.py:705(getter)\n",
      "    33003    0.244    0.000    0.349    0.000 python_message.py:775(_IsPresent)\n",
      "    17389    0.489    0.000    1.356    0.000 python_message.py:790(ListFields)\n",
      "    31827    0.161    0.000    0.161    0.000 python_message.py:792(<lambda>)\n",
      "      120    0.002    0.000    0.003    0.000 python_message.py:821(HasField)\n",
      "       60    0.001    0.000    0.002    0.000 python_message.py:894(Clear)\n",
      "        6    0.000    0.000    0.001    0.000 pywrap_tensorflow.py:354(TF_NewSessionOptions)\n",
      "       18    0.000    0.000    0.001    0.000 registry.py:70(lookup)\n",
      "     3000    0.171    0.000    0.234    0.000 samplers.py:12(update)\n",
      "      158    0.005    0.000    0.197    0.001 samplers.py:131(clean)\n",
      "     1000    0.005    0.000    0.005    0.000 samplers.py:141(reweight)\n",
      "        1    0.000    0.000    0.000    0.000 samplers.py:146(reset_counter)\n",
      "     6882    0.034    0.000    0.034    0.000 samplers.py:149(G)\n",
      "     3441    0.105    0.000    0.205    0.000 samplers.py:152(pred_g)\n",
      "     1000    0.109    0.000    0.364    0.000 samplers.py:155(sample_next)\n",
      "       12    0.001    0.000    0.015    0.001 samplers.py:221(refresh_v)\n",
      "      146    0.004    0.000    0.006    0.000 samplers.py:233(bounce)\n",
      "     1158    0.235    0.000    1.455    0.001 samplers.py:236(update_slope)\n",
      "      988    0.024    0.000    1.501    0.002 samplers.py:263(accept_or_reject)\n",
      "     1000    0.006    0.000    0.006    0.000 samplers.py:291(init)\n",
      "     1000    0.148    0.000    2.185    0.002 samplers.py:302(update)\n",
      "        3    0.000    0.000    0.000    0.000 samplers.py:8(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 samplers.py:88(__init__)\n",
      "        6    0.000    0.000    0.003    0.001 session.py:115(__init__)\n",
      "        6    0.000    0.000    0.001    0.000 session.py:155(close)\n",
      "        6    0.000    0.000    0.014    0.002 session.py:170(__del__)\n",
      "   154020    0.658    0.000    0.658    0.000 session.py:177(graph)\n",
      "    10009    0.099    0.000    0.389    0.000 session.py:196(as_default)\n",
      "    28015    0.142    0.000    0.142    0.000 session.py:282(<lambda>)\n",
      "    28997    0.136    0.000    0.136    0.000 session.py:283(<lambda>)\n",
      "    17009    0.577    0.000   52.570    0.003 session.py:287(run)\n",
      "    17009    0.239    0.000    0.548    0.000 session.py:495(_assert_fetchable)\n",
      "    17009    1.523    0.000    6.539    0.000 session.py:500(_process_fetches)\n",
      "    17009    0.422    0.000    0.747    0.000 session.py:502(_fetch_fn)\n",
      "    17009    4.284    0.000   51.741    0.003 session.py:553(_run)\n",
      "    28997    0.694    0.000    1.228    0.000 session.py:555(_feed_fn)\n",
      "    17009    0.078    0.000    0.078    0.000 session.py:581(<lambda>)\n",
      "    17009    0.236    0.000   21.781    0.001 session.py:666(_do_run)\n",
      "    17009    0.857    0.000   21.319    0.001 session.py:690(_run_fn)\n",
      "    17009    0.226    0.000   21.545    0.001 session.py:713(_do_call)\n",
      "    17009    0.218    0.000    7.615    0.000 session.py:730(_extend_graph)\n",
      "    17009    0.580    0.000    2.510    0.000 session.py:770(_update_with_movers)\n",
      "        6    0.000    0.000    0.004    0.001 session.py:858(__init__)\n",
      "    28997    0.526    0.000    1.407    0.000 session_ops.py:199(_get_handle_feeder)\n",
      "    28997    0.294    0.000    1.701    0.000 session_ops.py:219(_get_handle_mover)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:219(expand_dims)\n",
      "       18    0.000    0.000    0.000    0.000 six.py:598(iteritems)\n",
      "        7    0.000    0.000    0.000    0.000 stack_context.py:253(wrap)\n",
      "        6    0.000    0.000    0.006    0.001 state_ops.py:201(_AssignShape)\n",
      "       12    0.000    0.000    0.001    0.000 tensor_shape.py:110(merge_with)\n",
      "    51033    0.525    0.000    0.753    0.000 tensor_shape.py:28(__init__)\n",
      "   102054    1.084    0.000    2.252    0.000 tensor_shape.py:358(as_dimension)\n",
      "    29027    0.852    0.000    2.914    0.000 tensor_shape.py:417(__init__)\n",
      "    57994    0.237    0.000    0.237    0.000 tensor_shape.py:462(dims)\n",
      "    58018    0.483    0.000    0.707    0.000 tensor_shape.py:467(ndims)\n",
      "       12    0.000    0.000    0.000    0.000 tensor_shape.py:488(__getitem__)\n",
      "       18    0.001    0.000    0.004    0.000 tensor_shape.py:542(merge_with)\n",
      "        6    0.000    0.000    0.001    0.000 tensor_shape.py:595(assert_same_rank)\n",
      "    28997    1.237    0.000    7.370    0.000 tensor_shape.py:681(is_compatible_with)\n",
      "    58042    0.237    0.000    0.237    0.000 tensor_shape.py:74(value)\n",
      "        6    0.000    0.000    0.005    0.001 tensor_shape.py:765(as_proto)\n",
      "    51009    0.719    0.000    1.564    0.000 tensor_shape.py:79(is_compatible_with)\n",
      "    29027    0.414    0.000    3.457    0.000 tensor_shape.py:796(as_shape)\n",
      "       12    0.000    0.000    0.000    0.000 tensor_shape.py:804(unknown_shape)\n",
      "       12    0.000    0.000    0.001    0.000 tensor_shape.py:96(assert_is_compatible_with)\n",
      "        6    0.001    0.000    0.013    0.002 tensor_util.py:293(make_tensor_proto)\n",
      "       18    0.000    0.000    0.000    0.000 type_checkers.py:100(CheckValue)\n",
      "       30    0.000    0.000    0.001    0.000 type_checkers.py:118(CheckValue)\n",
      "     6096    0.108    0.000    0.181    0.000 type_checkers.py:162(CheckValue)\n",
      "      108    0.001    0.000    0.002    0.000 utf_8.py:15(decode)\n",
      "     4400    0.184    0.000    0.250    0.000 utils.py:6(iterate_minibatches)\n",
      "        6    0.000    0.000    0.150    0.025 variables.py:484(assign)\n",
      "        6    0.000    0.000    0.000    0.000 variables.py:636(initializer)\n",
      "        6    0.000    0.000    0.001    0.000 variables.py:835(all_variables)\n",
      "        6    0.000    0.000    0.034    0.006 variables.py:885(initialize_variables)\n",
      "        6    0.000    0.000    0.034    0.006 variables.py:911(initialize_all_variables)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import profile\n",
    "n_epochs=100\n",
    "batch_size=100\n",
    "total_iter=n_epochs*(N/batch_size)\n",
    "NLL_factor = 5\n",
    "profile.run('run_sam()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make plots pretty\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['font.family']='serif'\n",
    "plt.rcParams['mathtext.default']='regular'\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# Trajectories \n",
    "def gauss(x,y,hess,mu):\n",
    "    vec = np.asarray([x,y]) - np.asarray(mu)\n",
    "    return  -np.dot(vec,np.dot(hess,np.transpose(vec)))/2.0\n",
    "#x = np.linspace(0.4, .6, 100) #N=60000\n",
    "#y = np.linspace(2, 2.4, 100)\n",
    "x = np.linspace(W_map[0]-6*np.sqrt(cov_map[0,0]), W_map[0]+6*np.sqrt(cov_map[0,0]), 100)\n",
    "y = np.linspace(W_map[1]-6*np.sqrt(cov_map[1,1]), W_map[1]+6*np.sqrt(cov_map[1,1]), 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "zs = np.array([gauss(x,y,np.linalg.inv(cov_map[0:2,0:2]),[W_map[0][0],W_map[1][0]]) \\\n",
    "               for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "Z = zs.reshape(X.shape)\n",
    "CL = plt.contour(X, Y, Z,np.arange(-30, -5, 5))\n",
    "plt.clabel(CL, inline=1, fontsize=10)\n",
    "\n",
    "plt.title('Trajectories')\n",
    "ptx = []\n",
    "pty = []\n",
    "for i in range(10):\n",
    "    tx = np.random.multivariate_normal(np.ndarray.flatten(W_map),cov_map)\n",
    "    ptx.append(tx[0])\n",
    "    pty.append(tx[1])\n",
    "    #ptx.append(np.dot(tx,base_change)[0])\n",
    "    #pty.append(np.dot(tx,base_change)[1])\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "for i in range(len(trajectories)):\n",
    "    plt.plot(trajectories[i][:-1,0],trajectories[i][:-1,1],label=labels[i])\n",
    "#\n",
    "plt.xlabel('coordinate 0')\n",
    "plt.ylabel('coordinate 1')\n",
    "plt.scatter(ptx,pty,color='purple',label='Laplace samples')\n",
    "plt.scatter(W_map[0],W_map[1],label='MAP',color='red')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ACF(data,burnin):\n",
    "    corrs = []\n",
    "    for i in range(len(data)): \n",
    "        # empirical mean\n",
    "        #mean = []\n",
    "        #for k in range(total_iter-burnin):\n",
    "        #    mean.append(np.mean(trajectories[i][burnin:burnin+k,0]))\n",
    "        #means.append(mean)\n",
    "\n",
    "        #ACF\n",
    "        sig = data[i][burnin:,0] - np.mean(data[i][burnin:,0]) \n",
    "        corr = np.correlate(sig,sig,'full') \n",
    "        corr = corr/np.max(corr)\n",
    "        corr = corr[corr.shape[0]/2:] \n",
    "        corrs.append(corr)\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rotate_traj_ACF(trajectories,burnin):\n",
    "    base_change = np.linalg.eig(cov_map)[1]\n",
    "    corr2s = []\n",
    "    inds = [np.linalg.eig(cov_map)[0].argmax(),np.linalg.eig(cov_map)[0].argmin()]\n",
    "    wtrajectories = []\n",
    "    for i in range(len(trajectories)):\n",
    "        wtrajectories.append(np.dot(trajectories[i],base_change))\n",
    "    wwmap = np.squeeze(np.dot(np.transpose(W_map),base_change))\n",
    "    for j in range(2):\n",
    "        corrs3 = [] \n",
    "        for i in range(len(trajectories)): \n",
    "            #ACF\n",
    "            sig = wtrajectories[i][burnin/mbs[i]:,inds[j]] - np.mean(wtrajectories[i][burnin/mbs[i]:,inds[j]]) \n",
    "            corr = np.correlate(sig,sig,'full') \n",
    "            corr = corr/np.max(corr)\n",
    "            corr = corr[corr.shape[0]/2:] \n",
    "            corrs3.append(corr)\n",
    "        corr2s.append(corrs3)\n",
    "    return wtrajectories, corr2s, wwmap\n",
    "#[wtrajectories, corr2s, wwmap] = rotate_traj_ACF(trajectories,burnin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_plots(NLLs,labels,fname,corr2s,wtrajectories,wwmap,mbs):\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    plt.rcParams['font.family']='serif'\n",
    "    plt.rcParams['mathtext.default']='regular'\n",
    "    inds = [np.linalg.eig(cov_map)[0].argmax(),np.linalg.eig(cov_map)[0].argmin()]\n",
    "    #lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "    lines = [\"-\"]\n",
    "    alphas = [1,1,1,1,1,1,1]\n",
    "    linecycler = itertools.cycle(lines)\n",
    "    colors = ['b','g','r','y','m','orange','c']\n",
    "\n",
    "    # NLL plot\n",
    "    from matplotlib.ticker import FormatStrFormatter,AutoMinorLocator\n",
    "\n",
    "    chi_sq_m = D/float(2*N)\n",
    "    chi_sq_s = np.sqrt(D/float(2*N**2))\n",
    "    chi_sq_mode = max((D-2)/float(2*N),0)\n",
    "    plt.subplots(figsize=(20,10))\n",
    "    plt.subplot(231)\n",
    "\n",
    "    plt.title('NLL per data point (log scale)')\n",
    "    for j in range(len(NLLs)):\n",
    "        plt.loglog([mbs[j]*NLL_factor*i for i in range(len(NLLs[j]))],NLLs[j],\\\n",
    "                   next(linecycler),label=labels[j],color = colors[j],alpha = alphas[j])\n",
    "    max_len = mbs[0]*NLL_factor*len(NLLs[0])\n",
    "    min_len = NLL_factor*max(mbs)\n",
    "    plt.loglog([min_len,max_len],[NLL_map,NLL_map],label='NLL $_{\\hat{w}}/N$',color='black',linestyle=':')\n",
    "    plt.loglog([min_len,max_len],[NLL_map+chi_sq_m+chi_sq_s,NLL_map+chi_sq_m+chi_sq_s],color='black',linestyle='--',lw=2)\n",
    "    plt.loglog([min_len,max_len],[NLL_map+chi_sq_m-chi_sq_s,NLL_map+chi_sq_m-chi_sq_s],color='black',linestyle='--',lw=2)\n",
    "    plt.loglog([min_len,max_len],[NLL_map+chi_sq_m,NLL_map+chi_sq_m],\\\n",
    "                 label='Laplace approx.',color='black',lw=2)\n",
    "    #             label=r'$\\frac{1}{N}(\\frac{1}{2}\\chi^2(d)+NLL_{\\hat{w}})$',color='orange',lw=2)\n",
    "    plt.xlabel('data points observed')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    #ax.set_yscale('log')\n",
    "    #plt.tick_params(axis='y', which='minor')\n",
    "    # ax.yaxis.set_minor_formatter(FormatStrFormatter(\"%.2f\"))\n",
    "    #minor_locator = AutoMinorLocator(3)\n",
    "    #ax.yaxis.set_minor_locator(minor_locator)\n",
    "    plt.legend(bbox_to_anchor=(0, -.25), loc=2)\n",
    "    print max_len, NLL_map\n",
    "    plt.axis([min_len,max_len,0.95*NLL_map,1.05*np.max([np.max(NLL) for NLL in NLLs])])\n",
    "\n",
    "    # ACF\n",
    "    total_iter = max([len(wtrajectories[i])*mbs[i] for i in range(len(wtrajectories))])\n",
    "    j = 1\n",
    "    plt.subplot(232)\n",
    "    plt.title('direction of largest covariance')\n",
    "    for i in range(len(wtrajectories)): \n",
    "        x = np.asarray(range(len(corr2s[0][i])))\n",
    "        plt.semilogx(mbs[i]*x,corr2s[0][i],next(linecycler),label=labels[i],color = colors[i],alpha = alphas[i]) \n",
    "    plt.xlabel('lag')    \n",
    "    plt.ylabel('ACF')\n",
    "    plt.grid()\n",
    "    plt.axis([max(mbs)*10,total_iter-burnin,np.min([min(c) for c in corr2s[0]]),1])\n",
    "\n",
    "    # ACF\n",
    "    j = 1\n",
    "    plt.subplot(233)\n",
    "    plt.title('direction of smallest covariance')\n",
    "    for i in range(len(wtrajectories)): \n",
    "        x = np.asarray(range(len(corr2s[1][i])))\n",
    "        plt.semilogx(mbs[i]*x,corr2s[1][i],label=labels[i],color = colors[i],alpha = alphas[i]) \n",
    "    plt.xlabel('lag')    \n",
    "    plt.grid()\n",
    "    plt.axis([max(mbs)*10,total_iter-burnin,np.min([min(c) for c in corr2s[1]]),1])\n",
    "\n",
    "    # Coordinates (largest)\n",
    "    def plot_coord(j):\n",
    "        rng = 5*total_iter/5\n",
    "        std = np.sqrt(np.linalg.eig(cov_map)[0])[inds[j]]\n",
    "        for i in range(len(wtrajectories)): \n",
    "            x = np.asarray(range(len(wtrajectories[i][:rng/mbs[i],:])))\n",
    "            plt.plot(mbs[i]*x,wtrajectories[i][:rng/mbs[i],inds[j]],next(linecycler),\\\n",
    "                     color = colors[i],alpha = alphas[i])\n",
    "        plt.plot([0,rng],[wwmap[inds[j]],wwmap[inds[j]]],color='black',lw=2)\n",
    "        plt.plot([0,rng],[wwmap[inds[j]] + std,wwmap[inds[j]] + std],color='black',linestyle='--',lw=2)\n",
    "        plt.plot([0,rng],[wwmap[inds[j]] - std,wwmap[inds[j]] - std],color='black',linestyle='--',lw=2)\n",
    "        plt.xlabel('data points observed') \n",
    "        plt.locator_params(axis='x',nbins=4)\n",
    "        plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "    plt.subplot(235)\n",
    "    plot_coord(0)\n",
    "    plt.ylabel('coordinate trace')\n",
    "    #plt.ylim(15,35)\n",
    "    plt.subplot(236)\n",
    "    plot_coord(1)\n",
    "    std = np.sqrt(np.linalg.eig(cov_map)[0])[inds[j]]\n",
    "    plt.axis('tight')\n",
    "    plt.xlim(0,total_iter/20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    savefig(fname + '_d='+str(D),'../paper/arxiv/figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def savefig(name,dest):\n",
    "    fname = name + '.eps'\n",
    "    plt.savefig(fname, format='eps', dpi=1000, bbox_inches='tight')\n",
    "    os.system('epstopdf ' + fname)\n",
    "    os.system('pdfcrop ' + name + '.pdf')\n",
    "    os.system('cp ' + name + '-crop.pdf ' + dest + '/' + name + '.pdf')\n",
    "    os.system('rm ' + fname)\n",
    "    os.system('rm ' + name + '.pdf')\n",
    "    os.system('rm ' + name + '-crop.pdf')\n",
    "    print 'saved ' + dest + '/' + name + '.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "burnin = 3e6 # in data points \n",
    "[wtrajectories, corr2s, wwmap] = rotate_traj_ACF(trajectories,burnin)\n",
    "#all_plots(NLLs,labels,'lreg',[corr2s[0][:-2],corr2s[1][:-2]],wtrajectories[:-2],wwmap,mbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_plots(NLLs,labels,'lreg',corr2s,wtrajectories,wwmap,mbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run samplers\n",
    "n_epochs=1000\n",
    "batch_size=100\n",
    "NLL_factor = 5\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "total_iter=n_epochs*(N/batch_size)\n",
    "print('Running ' + str(n_epochs) + ' epochs (' +str(total_iter) + ' iterations) per method')\n",
    "fW_map = np.ndarray.flatten(W_map)\n",
    "\n",
    "#W_start = np.random.multivariate_normal(fW_map,cov_map)\n",
    "W_start = fW_map\n",
    "W_start = np.expand_dims(W_start,1)\n",
    "k = 3\n",
    "step_size = .1\n",
    "labels = []\n",
    "trajectories2 = []\n",
    "NLLs2 = []\n",
    "mbs = []\n",
    "\n",
    "# lSBPS\n",
    "k = 3\n",
    "print('Running lSBPS, k = ' + str(k))\n",
    "my_lSBPS = lSBPS(D,N,batch_size, [0,0],k)\n",
    "my_lSBPS.reset_counter()\n",
    "NLL_lSBPS_0,acc_lSBPS,samples_lSBPS_0 = runOptimizationExplicit(my_lSBPS,0,W_start)\n",
    "NLL_lSBPS,samples_lSBPS = generate_SBPS_samples(my_lSBPS,W_start,W,N)\n",
    "print('Number of bounces: ' + str(my_lSBPS.num_bounce))\n",
    "num_pacc = (my_lSBPS.p_errs/float(my_lSBPS.num_bounce))\n",
    "print('percent p(acc)>1: ', num_pacc)\n",
    "reject = (len(samples_lSBPS[:,0]) - my_lSBPS.num_bounce)/float(len(samples_lSBPS[:,0]))\n",
    "print('percent rejections: ', reject)\n",
    "print('percent negative slope: ', my_lSBPS.neg_S/float(len(my_lSBPS.all_Gs)))\n",
    "print 'total travel time: ', my_lSBPS.total_time \n",
    "labels.append('SBPS')\n",
    "trajectories2.append(samples_lSBPS)\n",
    "NLLs2.append(NLL_lSBPS)\n",
    "mbs.append(batch_size)\n",
    "\n",
    "## Run SGLD\n",
    "stepsizes = [1,.1,.01]\n",
    "for i in range(len(stepsizes)):\n",
    "    print('Running SGLD ', stepsizes )\n",
    "    total_iter=n_epochs*(N/batch_size)\n",
    "    my_SGLD = SGLD(N,stepsizes[i])\n",
    "    NLL_SGLD,acc_SGLD,samples_SGLD = runOptimizationExplicit(my_SGLD,0,W_start)\n",
    "    labels.append('SGLD ' + str(stepsizes[i]))\n",
    "    trajectories2.append(samples_SGLD)\n",
    "    NLLs2.append(NLL_SGLD)\n",
    "    mbs.append(batch_size)\n",
    "\n",
    "# Run mSGNHT\n",
    "batch_size=100\n",
    "total_iter=n_epochs*(N/batch_size)\n",
    "my_mSGNHT = mSGNHT(D,N,.1)\n",
    "print 'Running mSGNHT'\n",
    "NLL_mSGNHT,acc_mSGNHT,samples_mSGNHT = runOptimizationExplicit(my_mSGNHT,0,W_start)\n",
    "labels.append('mSGNHT')\n",
    "trajectories2.append(samples_mSGNHT)\n",
    "NLLs2.append(NLL_mSGNHT)\n",
    "mbs.append(batch_size)\n",
    "\n",
    "# lipschitz BPS\n",
    "print 'Running lipSBPS'\n",
    "batch_size=1\n",
    "total_iter=n_epochs*(N/batch_size)\n",
    "verbose=False\n",
    "L=np.sqrt(D)*np.max(np.abs(xData)) # this could be tightened, probably by a factor of 2\n",
    "my_bouncer = lipschitzSBPS(D,N,L,verbose)\n",
    "NLL_lipSBPS0,acc_lipSBPS,samples_lipSBPS0 = runOptimizationExplicit(my_bouncer,0,W_start)\n",
    "NLL_lipSBPS0 = 0\n",
    "acc_lipSBPS = 0\n",
    "my_bouncer.all_vs = [-v for v in my_bouncer.all_vs]\n",
    "NLL_lipSBPS,samples_lipSBPS = generate_SBPS_samples(my_bouncer,W_start,W,N)\n",
    "labels.append('lipSBPS')\n",
    "trajectories2.append(samples_lipSBPS)\n",
    "NLLs2.append(NLL_lipSBPS)\n",
    "mbs.append(batch_size)\n",
    "\n",
    "# Run ZZ-SS\n",
    "batch_size=1\n",
    "total_iter=n_epochs*(N/batch_size)\n",
    "logisticRegressionM=np.max(xData)\n",
    "my_zigzag_ss=ZZ_SS(D,N,logisticRegressionM)\n",
    "my_zigzag_ss.reset_counter()\n",
    "print('Running ZZ-SS')\n",
    "NLL_zz_ss0,acc_zz_ss,samples_zz0 = runOptimizationExplicit(my_zigzag_ss,0,W_start)\n",
    "acc_zz_ss = 0\n",
    "NLL_zz_ss0 = 0\n",
    "print('Number of flips: ' + str(my_zigzag_ss.flipcounter))\n",
    "NLL_zz_ss,samples_zz = generate_SBPS_samples(my_zigzag_ss,W_start,W,N)\n",
    "labels.append('ZZ-SS')\n",
    "trajectories2.append(samples_zz)\n",
    "NLLs2.append(NLL_zz_ss)\n",
    "mbs.append(1)\n",
    "\n",
    "wset=W.assign(W_map)\n",
    "with sess.as_default():\n",
    "    sess.run(wset)\n",
    "print 'MAP assigned, x coordinate is  - ', W_start[0]    \n",
    "NLL_map = sess.run(NLL, feed_dict={xDataPH: xData, yDataPH: yData})\n",
    "cond_num = np.max(np.linalg.eig(hessian_map)[0])/float(np.min(np.linalg.eig(hessian_map)[0]))\n",
    "print 'condition number ', cond_num\n",
    "stats = [total_iter,cond_num,my_lSBPS.num_bounce,my_lSBPS.p_errs/float(my_lSBPS.num_bounce),\\\n",
    "        (len(samples_lSBPS[:,0]) - my_lSBPS.num_bounce)/float(len(samples_lSBPS[:,0])),\\\n",
    "        my_lSBPS.neg_S/float(len(my_lSBPS.all_Gs)),my_lSBPS.total_time]\n",
    "        # my_plSBPS.num_bounce,my_plSBPS.p_errs/float(my_plSBPS.num_bounce),\\\n",
    "        #(len(samples_plSBPS[:,0]) - my_plSBPS.num_bounce)/float(len(samples_plSBPS[:,0])),\\\n",
    "        #my_plSBPS.neg_S/float(len(my_plSBPS.all_Gs)),my_plSBPS.total_time]\n",
    "#np.save('samples_' + str(D),[stats,trajectories,NLLs,labels,NLL_map])\n",
    "#print 'saved samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[wtrajectories2, corr2s2, wwmap2] = rotate_traj_ACF(trajectories2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "batch_size=100\n",
    "total_iter=n_epochs*(N/batch_size)\n",
    "print('Running lSBPS, k = ' + str(k))\n",
    "my_lSBPS = lSBPS(D,N,batch_size, [0,0],k)\n",
    "my_lSBPS.reset_counter()\n",
    "NLL_lSBPS_0,acc_lSBPS,samples_lSBPS_0 = runOptimizationExplicit(my_lSBPS,0,W_start)\n",
    "NLL_lSBPS,samples_lSBPS = generate_SBPS_samples(my_lSBPS,W_start,W,N)\n",
    "print('Number of bounces: ' + str(my_lSBPS.num_bounce))\n",
    "num_pacc = (my_lSBPS.p_errs/float(my_lSBPS.num_bounce))\n",
    "print('percent p(acc)>1: ', num_pacc)\n",
    "reject = (len(samples_lSBPS[:,0]) - my_lSBPS.num_bounce)/float(len(samples_lSBPS[:,0]))\n",
    "print('percent rejections: ', reject)\n",
    "print('percent negative slope: ', my_lSBPS.neg_S/float(len(my_lSBPS.all_Gs)))\n",
    "print 'total travel time: ', my_lSBPS.total_time \n",
    "trajectories2[0] = samples_lSBPS\n",
    "NLLs2[0] = NLL_lSBPS\n",
    "burnin = 0\n",
    "base_change = np.linalg.eig(cov_map)[1]\n",
    "corr2s3 = []\n",
    "inds = [np.linalg.eig(cov_map)[0].argmax(),np.linalg.eig(cov_map)[0].argmin()]\n",
    "wtrajectories3 = []\n",
    "for i in range(len(trajectories)):\n",
    "    wtrajectories3.append(np.dot(trajectories2[i],base_change))\n",
    "wwmap = np.squeeze(np.dot(np.transpose(W_map),base_change))\n",
    "for j in range(2):\n",
    "    corrs3 = [] \n",
    "    for i in range(1): \n",
    "        #ACF\n",
    "        sig = wtrajectories3[i][burnin/mbs[i]:,inds[j]] - np.mean(wtrajectories3[i][burnin/mbs[i]:,inds[j]]) \n",
    "        corr = np.correlate(sig,sig,'full') \n",
    "        corr = corr/np.max(corr)\n",
    "        corr = corr[corr.shape[0]/2:] \n",
    "        corrs3.append(corr)\n",
    "    corr2s3.append(corrs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = 5*total_iter/5\n",
    "inds = [np.linalg.eig(cov_map)[0].argmax(),np.linalg.eig(cov_map)[0].argmin()]\n",
    "j = 0\n",
    "for i in range(len(wtrajectories2)): \n",
    "    x = np.asarray(range(len(wtrajectories[i][:rng/mbs[i],:])))\n",
    "    plt.plot(mbs[i]*x,wtrajectories2[i][:rng/mbs[i],inds[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_plots(NLLs[:1] + NLLs[2:],labels[:1]+labels[2:],'lreg', [corr2s2[0][:1] + corr2s2[0][2:],corr2s2[1][:1] + corr2s2[1][2:]]\\\n",
    "          ,wtrajectories[:1] + wtrajectories[2:],wwmap,mbs[:1] + mbs[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
